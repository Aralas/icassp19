{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\n",
      "  from numba.decorators import jit as optional_jit\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm, trange\n",
    "import time\n",
    "import pprint\n",
    "import datetime\n",
    "import argparse\n",
    "from scipy.stats import gmean\n",
    "import yaml\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "import utils\n",
    "from feat_ext import load_audio_file, get_mel_spectrogram, modify_file_variable_length\n",
    "from data import get_label_files, DataGeneratorPatch, PatchGeneratorPerFile, DataGeneratorPatchOrigin\n",
    "from architectures import get_model_baseline, get_model_binary\n",
    "from eval import Evaluator\n",
    "from losses import lq_loss_wrap, crossentropy_max_wrap, crossentropy_outlier_wrap, crossentropy_reed_wrap,\\\n",
    "    crossentropy_max_origin_wrap, crossentropy_outlier_origin_wrap, lq_loss_origin_wrap, crossentropy_reed_origin_wrap\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current date and time:\n",
      "2020-04-27 02:44:35.730900\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "print(\"Current date and time:\")\n",
    "print(str(now))\n",
    "\n",
    "\n",
    "\n",
    "# =========================================================================Parameters, paths and variables\n",
    "# =========================================================================Parameters, paths and variables\n",
    "# =========================================================================Parameters, paths and variables\n",
    "\n",
    "# Read parameters file from yaml passed by argument\n",
    "params = yaml.load(open('config/params.yaml'))\n",
    "params_ctrl = params['ctrl']\n",
    "params_extract = params['extract']\n",
    "params_learn = params['learn']\n",
    "params_loss = params['loss']\n",
    "params_recog = params['recognizer']\n",
    "\n",
    "suffix_in = params['suffix'].get('in')\n",
    "suffix_out = params['suffix'].get('out')\n",
    "\n",
    "\n",
    "# determine loss function\n",
    "flag_origin = False\n",
    "if params_loss.get('type') == 'CCE':\n",
    "    params_loss['type'] = 'categorical_crossentropy'\n",
    "elif params_loss.get('type') == 'lq_loss':\n",
    "    params_loss['type'] = lq_loss_wrap(params_loss.get('q_loss'))\n",
    "elif params_loss.get('type') == 'CCE_max':\n",
    "    params_loss['type'] = crossentropy_max_wrap(params_loss.get('m_loss'))\n",
    "elif params_loss.get('type') == 'CCE_outlier':\n",
    "    params_loss['type'] = crossentropy_outlier_wrap(params_loss.get('l_loss'))\n",
    "elif params_loss.get('type') == 'bootstrapping':\n",
    "    params_loss['type'] = crossentropy_reed_wrap(params_loss.get('reed_beta'))\n",
    "\n",
    "# selective loss based on data origin\n",
    "elif params_loss.get('type') == 'CCE_max_origin':\n",
    "    params_loss['type'] = crossentropy_max_origin_wrap(params_loss.get('m_loss'))\n",
    "    flag_origin = True\n",
    "elif params_loss.get('type') == 'CCE_outlier_origin':\n",
    "    params_loss['type'] = crossentropy_outlier_origin_wrap(params_loss.get('l_loss'))\n",
    "    flag_origin = True\n",
    "elif params_loss.get('type') == 'lq_loss_origin':\n",
    "    params_loss['type'] = lq_loss_origin_wrap(params_loss.get('q_loss'))\n",
    "    flag_origin = True\n",
    "elif params_loss.get('type') == 'bootstrapping_origin':\n",
    "    params_loss['type'] = crossentropy_reed_origin_wrap(params_loss.get('reed_beta'))\n",
    "    flag_origin = True\n",
    "\n",
    "\n",
    "params_extract['audio_len_samples'] = int(params_extract.get('fs') * params_extract.get('audio_len_s'))\n",
    "#\n",
    "\n",
    "# ======================================================== PATHS FOR DATA, FEATURES and GROUND TRUTH\n",
    "# where to look for the dataset\n",
    "path_root_data = params_ctrl.get('dataset_path')\n",
    "\n",
    "params_path = {'path_to_features': os.path.join(path_root_data, 'features'),\n",
    "               'featuredir_tr': 'audio_train_varup2/',\n",
    "               'featuredir_te': 'audio_test_varup2/',\n",
    "               'path_to_dataset': path_root_data,\n",
    "               'audiodir_tr': 'FSDnoisy18k.audio_train/',\n",
    "               'audiodir_te': 'FSDnoisy18k.audio_test/',\n",
    "               'audio_shapedir_tr': 'audio_train_shapes/',\n",
    "               'audio_shapedir_te': 'audio_test_shapes/',\n",
    "               'gt_files': os.path.join(path_root_data, 'FSDnoisy18k.meta')}\n",
    "\n",
    "\n",
    "params_path['featurepath_tr'] = os.path.join(params_path.get('path_to_features'), params_path.get('featuredir_tr'))\n",
    "params_path['featurepath_te'] = os.path.join(params_path.get('path_to_features'), params_path.get('featuredir_te'))\n",
    "\n",
    "params_path['audiopath_tr'] = os.path.join(params_path.get('path_to_dataset'), params_path.get('audiodir_tr'))\n",
    "params_path['audiopath_te'] = os.path.join(params_path.get('path_to_dataset'), params_path.get('audiodir_te'))\n",
    "\n",
    "params_path['audio_shapepath_tr'] = os.path.join(params_path.get('path_to_dataset'),\n",
    "                                                 params_path.get('audio_shapedir_tr'))\n",
    "params_path['audio_shapepath_te'] = os.path.join(params_path.get('path_to_dataset'),\n",
    "                                                 params_path.get('audio_shapedir_te'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "params_ctrl=\n",
      "{   'dataset_path': '/data/FSDnoisy/',\n",
      "    'feat_ext': False,\n",
      "    'learn': True,\n",
      "    'train_data': 'all'}\n",
      "params_files=\n",
      "{   'gt_test': '/data/FSDnoisy/FSDnoisy18k.meta/test.csv',\n",
      "    'gt_train': '/data/FSDnoisy/FSDnoisy18k.meta/train.csv'}\n",
      "params_extract=\n",
      "{   'audio_len_s': 2,\n",
      "    'audio_len_samples': 88200,\n",
      "    'eps': 2.220446049250313e-16,\n",
      "    'fmax': 22050,\n",
      "    'fmin': 0,\n",
      "    'fs': 44100,\n",
      "    'hop_length_samples': 882,\n",
      "    'load_mode': 'varup',\n",
      "    'log': True,\n",
      "    'mono': True,\n",
      "    'n_fft': 2048,\n",
      "    'n_mels': 96,\n",
      "    'normalize_audio': True,\n",
      "    'patch_hop': 50,\n",
      "    'patch_len': 100,\n",
      "    'spectrogram_type': 'power',\n",
      "    'win_length_samples': 1764}\n",
      "params_learn=\n",
      "{   'batch_size': 64,\n",
      "    'lr': 0.001,\n",
      "    'n_classes': 20,\n",
      "    'n_epochs': 20,\n",
      "    'optim': 'Adam',\n",
      "    'patience': 15,\n",
      "    'val_split': 0.15}\n",
      "params_loss=\n",
      "{   'l_loss': 1.9,\n",
      "    'm_loss': 0.6,\n",
      "    'q_loss': 0.7,\n",
      "    'reed_beta': 0.3,\n",
      "    'type': 'categorical_crossentropy'}\n",
      "params_recog=\n",
      "{   'aggregate': 'gmean'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# ======================================================== SPECIFIC PATHS TO SOME IMPORTANT FILES\n",
    "# ground truth, load model, save model, predictions, results\n",
    "params_files = {'gt_test': os.path.join(params_path.get('gt_files'), 'test.csv'),\n",
    "                'gt_train': os.path.join(params_path.get('gt_files'), 'train.csv')}\n",
    "\n",
    "# # ============================================= print all params to keep record in output file\n",
    "print('\\nparams_ctrl=')\n",
    "pprint.pprint(params_ctrl, width=1, indent=4)\n",
    "print('params_files=')\n",
    "pprint.pprint(params_files, width=1, indent=4)\n",
    "print('params_extract=')\n",
    "pprint.pprint(params_extract, width=1, indent=4)\n",
    "print('params_learn=')\n",
    "pprint.pprint(params_learn, width=1, indent=4)\n",
    "print('params_loss=')\n",
    "pprint.pprint(params_loss, width=1, indent=4)\n",
    "print('params_recog=')\n",
    "pprint.pprint(params_recog, width=1, indent=4)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv(params_files.get('gt_train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Walk_or_footsteps', 'Fireworks', 'Clapping', 'Rain', 'Bass_guitar', 'Crash_cymbal', 'Wind', 'Coin_(dropping)', 'Hi-hat', 'Fart', 'Tearing', 'Engine', 'Writing', 'Slam', 'Squeak', 'Piano', 'Glass', 'Acoustic_guitar', 'Dishes_and_pots_and_pans', 'Fire'}\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76]\n",
      "<class 'list'> <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "clean_index = train_csv.manually_verified.values.tolist()\n",
    "labels = train_csv.label.values.tolist()\n",
    "print(set(labels))\n",
    "label_list = list(set(labels))\n",
    "print(np.where((np.array(labels)==label_list[0]) & (np.array(clean_index)==1))[0])\n",
    "print(type(labels),type(label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv_new = train_csv.reindex(columns=['fname','label','aso_id','manually_verified','noisy_small'\n",
    "]+[str(i) for i in range(20)], fill_value=0)\n",
    "for i in range(20):\n",
    "    index=np.where((np.array(labels)==list_labels[i]) & (np.array(clean_index)==1))[0]\n",
    "    train_csv_new.iloc[index,i+5]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'record/generate_clean_data/train.csv'\n",
    "train_csv_new.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Acoustic_guitar', 'Bass_guitar', 'Clapping', 'Coin_(dropping)', 'Crash_cymbal', 'Dishes_and_pots_and_pans', 'Engine', 'Fart', 'Fire', 'Fireworks', 'Glass', 'Hi-hat', 'Piano', 'Rain', 'Slam', 'Squeak', 'Tearing', 'Walk_or_footsteps', 'Wind', 'Writing']\n",
      "['/m/018vs', '/m/0242l', '/m/02_41', '/m/02_nn', '/m/02mk9', '/m/039jq', '/m/03m9d0z', '/m/03qtq', '/m/042v_gx', '/m/04brg2', '/m/05r5c', '/m/06mb1', '/m/07pbtc8', '/m/07q6cd_', '/m/07qcx4z', '/m/07rjzl8', '/m/081rb', '/m/0bm0k', '/m/0g6b5', '/m/0l15bq']\n"
     ]
    }
   ],
   "source": [
    "list_labels = sorted(list(set(train_csv.label.values)))\n",
    "list_aso_ids = sorted(list(set(train_csv.aso_id.values)))\n",
    "print(list_labels)\n",
    "print(list_aso_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'record/generate_clean_data/train.csv'\n",
    "train_csv_clean = pd.read_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "[12651 12652 12653 12654 12655 12656 12657 12658 12659 12660 12661 12662\n",
      " 12663 12664 12665 12666 12667 12668 12669 12670 12671 12672 12673 12674\n",
      " 12675 12676 12677 12678 12679 12680 12681 12682 12683 12684 12685 12686\n",
      " 12687 12688 12689 12690 12691 12692 12693 12694 12695 12696 12697 12698\n",
      " 12699 12700 12701 12702 12703 12704 12705 12706 12707 12708 12709 12710\n",
      " 12711 12712 12713 12714 12715 12716 12717 12718 12719 12720 12721 12722\n",
      " 12723 12724 12725 12726 12727 12728 12729 12730 12731 12732 12733 12734\n",
      " 12735 12736 12737 12738 12739 12740 12741 12742 12743 12744 12745 12746\n",
      " 12747 12748 12749 12750 12751 12752]\n"
     ]
    }
   ],
   "source": [
    "target_label=0\n",
    "binary_labels = np.array(train_csv_clean[str(target_label)])\n",
    "print(binary_labels)\n",
    "positive_list = np.where(binary_labels==1)[0]\n",
    "negative_list = np.where(binary_labels==0)[0]\n",
    "print(positive_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Acoustic_guitar\n",
      "1 Bass_guitar\n",
      "2 Clapping\n",
      "3 Coin_(dropping)\n",
      "4 Crash_cymbal\n",
      "5 Dishes_and_pots_and_pans\n",
      "6 Engine\n",
      "7 Fart\n",
      "8 Fire\n",
      "9 Fireworks\n",
      "10 Glass\n",
      "11 Hi-hat\n",
      "12 Piano\n",
      "13 Rain\n",
      "14 Slam\n",
      "15 Squeak\n",
      "16 Tearing\n",
      "17 Walk_or_footsteps\n",
      "18 Wind\n",
      "19 Writing\n"
     ]
    }
   ],
   "source": [
    "file = 'record/generate_clean_data/train.csv'\n",
    "data = pd.read_csv(file)\n",
    "for label in range(20):\n",
    "    binary_labels = np.array(data[str(label)])\n",
    "    index = np.where(binary_labels==1)[0]\n",
    "    print(label, data.label.values[index[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Acoustic_guitar 463\n",
      "1 Bass_guitar 542\n",
      "2 Clapping 473\n",
      "3 Coin_(dropping) 585\n",
      "4 Crash_cymbal 451\n",
      "5 Dishes_and_pots_and_pans 503\n",
      "6 Engine 531\n",
      "7 Fart 317\n",
      "8 Fire 413\n",
      "9 Fireworks 320\n",
      "10 Glass 523\n",
      "11 Hi-hat 570\n",
      "12 Piano 457\n",
      "13 Rain 425\n",
      "14 Slam 349\n",
      "15 Squeak 323\n",
      "16 Tearing 388\n",
      "17 Walk_or_footsteps 459\n",
      "18 Wind 362\n",
      "19 Writing 426\n",
      "10652 6880\n"
     ]
    }
   ],
   "source": [
    "file_name = 'record/generate_clean_data/thres_0.90_crit_9/iteration%d.csv'%iteration\n",
    "data = pd.read_csv(file_name)\n",
    "list_labels = sorted(list(set(data.label.values)))\n",
    "labels = np.array(data.label.values)\n",
    "index_all = list(np.where(np.array(data.manually_verified.values)==1)[0])\n",
    "for label in range(20):\n",
    "    index = np.where(np.array(data[str(label)])==1)[0]\n",
    "    print(label, list_labels[label], len(index))\n",
    "    index_all += list(index)\n",
    "    labels[index] = list_labels[label]\n",
    "print(len(index_all), len(set(index_all)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data.iloc[list(set(index_all)), 0:5]\n",
    "data1.to_csv('record/generate_clean_data/thres_0.90_crit_9/teacher.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1022.  696.  663.  587.  671.]\n",
      " [ 458.  602.  673.  822.  765.]\n",
      " [ 445.  513.  642.  663.  695.]\n",
      " [ 415.  522.  614.  746.  761.]\n",
      " [  76.   76.   76.   76.   76.]]\n",
      "[[588. 377. 493. 424. 463.]\n",
      " [394. 506. 588. 544. 542.]\n",
      " [320. 250. 415. 471. 473.]\n",
      " [402. 455. 475. 543. 585.]\n",
      " [285. 362. 387. 439. 451.]\n",
      " [150. 366. 306. 456. 503.]\n",
      " [327. 390. 362. 405. 531.]\n",
      " [ 71. 151. 165. 313. 317.]\n",
      " [183. 228. 458. 372. 413.]\n",
      " [161. 221. 289. 401. 320.]\n",
      " [370. 353. 518. 488. 523.]\n",
      " [599. 291. 644. 435. 570.]\n",
      " [589. 322. 382. 568. 457.]\n",
      " [ 74. 252. 295. 364. 425.]\n",
      " [ 96. 172. 213. 304. 349.]\n",
      " [135. 405. 439. 408. 323.]\n",
      " [473. 154. 309. 273. 388.]\n",
      " [271. 316. 315. 343. 459.]\n",
      " [212. 368. 332. 321. 362.]\n",
      " [288. 275. 402. 279. 426.]]\n",
      "[[565. 114. 388. 165. 260.]\n",
      " [276. 253. 275. 290. 303.]\n",
      " [132. 168. 196. 171. 261.]\n",
      " [257. 283. 285. 356. 325.]\n",
      " [147. 104. 165. 141. 178.]\n",
      " [304.  96. 145. 162. 170.]\n",
      " [188. 279. 218. 281. 290.]\n",
      " [ 71.  79.  97. 166.  97.]\n",
      " [ 84. 103. 116. 113. 115.]\n",
      " [ 51.  59.  54.  71.  82.]\n",
      " [223. 231. 247. 157. 157.]\n",
      " [ 81.  81.  81.  81.  81.]\n",
      " [ 96.  96.  96.  96.  96.]\n",
      " [ 65.  65.  65.  65.  65.]\n",
      " [ 78.  78.  78.  78.  78.]\n",
      " [ 67.  67.  67.  67.  67.]\n",
      " [ 62.  62.  62.  62.  62.]\n",
      " [ 77.  77.  77.  77.  77.]\n",
      " [ 64.  64.  64.  64.  64.]\n",
      " [ 66.  66.  66.  66.  66.]]\n",
      "[[533. 278. 414. 389. 387.]\n",
      " [370. 379. 457. 405. 412.]\n",
      " [191. 248. 311. 295. 262.]\n",
      " [250. 355. 407. 361. 457.]\n",
      " [136. 258. 337. 302. 373.]\n",
      " [106. 210. 252. 291. 340.]\n",
      " [318. 381. 399. 323. 364.]\n",
      " [ 71. 254. 194. 282. 351.]\n",
      " [ 96. 127. 132. 182. 153.]\n",
      " [ 73.  82.  91. 154. 186.]\n",
      " [321. 289. 381. 456. 451.]\n",
      " [160. 390. 414. 425. 358.]\n",
      " [415. 220. 361. 271. 379.]\n",
      " [ 81. 173. 215. 241. 210.]\n",
      " [ 78. 177. 169. 171. 230.]\n",
      " [ 77.  91. 257. 175. 217.]\n",
      " [632. 101. 221. 212. 226.]\n",
      " [413. 150. 278. 296. 337.]\n",
      " [ 72. 214. 226. 182. 233.]\n",
      " [272. 196. 298. 245. 212.]]\n"
     ]
    }
   ],
   "source": [
    "tabel = np.zeros([5, 5])\n",
    "for iteration in range(5):\n",
    "    file_name = 'record/generate_clean_data/thres_0.80_crit_9/iteration%d.csv'%iteration\n",
    "    data = pd.read_csv(file_name)\n",
    "    for label in range(5):\n",
    "        tabel[label, iteration] = sum(data[str(label)])\n",
    "print(tabel)\n",
    "\n",
    "tabel = np.zeros([20, 5])\n",
    "for iteration in range(5):\n",
    "    file_name = 'record/generate_clean_data/thres_0.90_crit_9/iteration%d.csv'%iteration\n",
    "    data = pd.read_csv(file_name)\n",
    "    for label in range(20):\n",
    "        tabel[label, iteration] = sum(data[str(label)])\n",
    "print(tabel)\n",
    "\n",
    "tabel = np.zeros([20, 5])\n",
    "for iteration in range(5):\n",
    "    file_name = 'record/generate_clean_data/thres_0.95_crit_9_lr_0.0001/iteration%d.csv'%iteration\n",
    "    data = pd.read_csv(file_name)\n",
    "    for label in range(20):\n",
    "        tabel[label, iteration] = sum(data[str(label)])\n",
    "print(tabel)\n",
    "\n",
    "tabel = np.zeros([20, 5])\n",
    "for iteration in range(5):\n",
    "    file_name = 'record/generate_clean_data/thres_0.95_crit_9/iteration%d.csv'%iteration\n",
    "    data = pd.read_csv(file_name)\n",
    "    for label in range(20):\n",
    "        tabel[label, iteration] = sum(data[str(label)])\n",
    "print(tabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 102\n",
      "1 106\n",
      "2 82\n",
      "3 170\n",
      "4 76\n",
      "5 83\n",
      "6 137\n",
      "7 71\n",
      "8 81\n",
      "9 51\n",
      "10 157\n",
      "11 81\n",
      "12 96\n",
      "13 65\n",
      "14 78\n",
      "15 67\n",
      "16 62\n",
      "17 77\n",
      "18 64\n",
      "19 66\n"
     ]
    }
   ],
   "source": [
    "file_name = 'record/generate_clean_data/train.csv'\n",
    "data = pd.read_csv(file_name)\n",
    "for label in range(20):\n",
    "    print(label,sum(data[str(label)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.48891235480464\n"
     ]
    }
   ],
   "source": [
    "for lr in [0.0001]:\n",
    "    f = open('record/benchmark/benchmark_lr_%.5f.txt' % lr)\n",
    "    acc = []\n",
    "    lines = f.readlines()\n",
    "    for i in range(1, 171):\n",
    "        line = lines[i].strip().split(',')\n",
    "        acc.append(float(line[-1]))\n",
    "    print(np.max(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.223864836325237\n",
      "67.37064413938754\n",
      "68.74340021119323\n",
      "68.74340021119323\n",
      "65.46990496304119\n"
     ]
    }
   ],
   "source": [
    "for lr in [0.01, 0.001, 0.0003, 0.0001, 0.00003]:\n",
    "    f = open('record/teacher/teacher_lr_%.5f.txt' % lr)\n",
    "    acc = []\n",
    "    lines = f.readlines()\n",
    "    for i in range(1, 201):\n",
    "        line = lines[i].strip().split(',')\n",
    "        acc.append(float(line[-1]))\n",
    "    print(np.max(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch, loss, training accuracy, test accuracy0,3.1398800213020164,0.10019668,16.68426610348469\n",
      " ['epoch', ' loss', ' training accuracy', ' test accuracy0', '3.1398800213020164', '0.10019668', '16.68426610348469']\n"
     ]
    }
   ],
   "source": [
    "print(lines[0], lines[0].strip().split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
